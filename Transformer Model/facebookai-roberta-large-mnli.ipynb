{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":791526,"sourceType":"datasetVersion","datasetId":413654,"isSourceIdPinned":false},{"sourceId":601330,"sourceType":"modelInstanceVersion","modelInstanceId":450632,"modelId":466984}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ls -R /kaggle/input/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T07:45:45.321965Z","iopub.execute_input":"2025-10-15T07:45:45.322346Z","iopub.status.idle":"2025-10-15T07:45:45.474955Z","shell.execute_reply.started":"2025-10-15T07:45:45.322324Z","shell.execute_reply":"2025-10-15T07:45:45.473856Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/:\nroberta-large-mnli-local  twitter-climate-change-sentiment-dataset\n\n/kaggle/input/roberta-large-mnli-local:\ntransformers\n\n/kaggle/input/roberta-large-mnli-local/transformers:\ndefault\n\n/kaggle/input/roberta-large-mnli-local/transformers/default:\n1\n\n/kaggle/input/roberta-large-mnli-local/transformers/default/1:\nroberta-large-mnli-local\n\n/kaggle/input/roberta-large-mnli-local/transformers/default/1/roberta-large-mnli-local:\nconfig.json  model.safetensors\t      tokenizer_config.json  vocab.json\nmerges.txt   special_tokens_map.json  tokenizer.json\n\n/kaggle/input/twitter-climate-change-sentiment-dataset:\ntwitter_sentiment_data.csv\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport os\n\n# Path yang benar berdasarkan struktur folder Anda\nmodel_path = \"/kaggle/input/roberta-large-mnli-local/transformers/default/1/roberta-large-mnli-local\"\n\nprint(f\"Menggunakan path: {model_path}\")\n\n# Me-load tokenizer dan model dari path yang sudah benar\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\n\nprint(\"\\nModel dan tokenizer berhasil dimuat! ✅\")\n\n# Contoh penggunaan\ntext = \"The new movie was absolutely fantastic!\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model(**inputs)\n\nprint(\"\\nContoh inferensi berhasil dijalankan.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T07:45:45.476867Z","iopub.execute_input":"2025-10-15T07:45:45.477245Z","iopub.status.idle":"2025-10-15T07:45:46.107904Z","shell.execute_reply.started":"2025-10-15T07:45:45.477217Z","shell.execute_reply":"2025-10-15T07:45:46.107096Z"}},"outputs":[{"name":"stdout","text":"Menggunakan path: /kaggle/input/roberta-large-mnli-local/transformers/default/1/roberta-large-mnli-local\n\nModel dan tokenizer berhasil dimuat! ✅\n\nContoh inferensi berhasil dijalankan.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\nparams_bytes = total_params * 4 #FP32 = 4 bytes\nparams_size_gb = params_bytes / (1024**3)\nprint(total_params)\nprint(params_size_gb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T07:45:46.108877Z","iopub.execute_input":"2025-10-15T07:45:46.109174Z","iopub.status.idle":"2025-10-15T07:45:46.116942Z","shell.execute_reply.started":"2025-10-15T07:45:46.109149Z","shell.execute_reply":"2025-10-15T07:45:46.116202Z"}},"outputs":[{"name":"stdout","text":"355362819\n1.3238296620547771\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nimport os\n\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom datasets import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom transformers import AutoConfig\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T07:45:46.118759Z","iopub.execute_input":"2025-10-15T07:45:46.119006Z","iopub.status.idle":"2025-10-15T07:45:46.131023Z","shell.execute_reply.started":"2025-10-15T07:45:46.118991Z","shell.execute_reply":"2025-10-15T07:45:46.130389Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"url = \"https://raw.githubusercontent.com/NafisNaufal/climate-change-sentiment-analysis/add-file-intern/cleaned_tweets.csv\"\ndf = pd.read_csv(url)\n\nprint(df.head())\n\ndf_clean = df_clean.dropna(subset=['message', 'sentiment']).reset_index(drop=True)\ndf_clean = df_clean[df_clean['sentiment'] != 2].reset_index(drop=True)\n\ntrain_df, test_df = train_test_split(df_clean, test_size=0.2, random_state=42, stratify=df_clean['sentiment'])\n\nsentiments = df_clean['sentiment'].unique().tolist()\nsentiment2id = {sentiment: i for i, sentiment in enumerate(sentiments)}\nid2sentiment = {i: sentiment for i, sentiment in enumerate(sentiments)}\n\ntrain_df['sentiment_encoded'] = train_df['sentiment'].map(sentiment2id)\ntest_df['sentiment_encoded'] = test_df['sentiment'].map(sentiment2id)\n\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T07:45:46.131839Z","iopub.execute_input":"2025-10-15T07:45:46.132080Z","iopub.status.idle":"2025-10-15T07:45:46.400414Z","shell.execute_reply.started":"2025-10-15T07:45:46.132061Z","shell.execute_reply":"2025-10-15T07:45:46.399578Z"}},"outputs":[{"name":"stdout","text":"                                             message  sentiment\n0  @tiniebeany climate change is an interesting h...         -1\n1  Watch #BeforeTheFlood right here, as @LeoDiCap...          1\n2  Fabulous! Leonardo #DiCaprio's film on #climat...          1\n3  Just watched this amazing documentary by leona...          1\n4  Leonardo DiCaprio's climate change documentary...          0\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def tokenize_function(examples):\n  return tokenizer(examples['message'], padding=\"max_length\", truncation=True, max_length=128)\n\ntokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\ntokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n\ntokenized_train_dataset = tokenized_train_dataset.remove_columns(['sentiment', 'message', '__index_level_0__'])\ntokenized_test_dataset = tokenized_test_dataset.remove_columns(['sentiment', 'message', '__index_level_0__'])\ntokenized_train_dataset = tokenized_train_dataset.rename_column('sentiment_encoded', 'labels')\ntokenized_test_dataset = tokenized_test_dataset.rename_column('sentiment_encoded', 'labels')\n\n\ntokenized_train_dataset.set_format('torch')\ntokenized_test_dataset.set_format('torch')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T07:45:46.401277Z","iopub.execute_input":"2025-10-15T07:45:46.401565Z","iopub.status.idle":"2025-10-15T07:45:49.882048Z","shell.execute_reply.started":"2025-10-15T07:45:46.401524Z","shell.execute_reply":"2025-10-15T07:45:49.881332Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/24248 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6f07f1c64cd49668a6edaf69e0ad7b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6062 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9569d33a47574a3d8b9eb447690843fe"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"train_dataloader = DataLoader(tokenized_train_dataset, shuffle=True, batch_size=16)\neval_dataloader = DataLoader(tokenized_test_dataset, shuffle=False, batch_size=16)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T07:45:49.882878Z","iopub.execute_input":"2025-10-15T07:45:49.883146Z","iopub.status.idle":"2025-10-15T07:45:49.888262Z","shell.execute_reply.started":"2025-10-15T07:45:49.883124Z","shell.execute_reply":"2025-10-15T07:45:49.887585Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n\nnum_epochs = 3\nnum_training_steps = num_epochs * len(train_dataloader)\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T07:45:49.889204Z","iopub.execute_input":"2025-10-15T07:45:49.889491Z","iopub.status.idle":"2025-10-15T07:45:49.963434Z","shell.execute_reply.started":"2025-10-15T07:45:49.889465Z","shell.execute_reply":"2025-10-15T07:45:49.962667Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"%%writefile train_script.py\n\nimport os\nimport torch\nimport pandas as pd\nimport numpy as np\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.cuda.amp import GradScaler, autocast\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom datasets import Dataset\nfrom torch.optim import AdamW\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\nimport kagglehub\n\ndef main_worker(rank, world_size, model_path, num_epochs, batch_size, accumulation_steps):\n    print(f\"Running DDP on rank {rank}.\")\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n    model.to(rank)\n    ddp_model = DDP(model, device_ids=[rank])\n\n    # --- Persiapan Dataset (tidak ada perubahan) ---\n    path = kagglehub.dataset_download(\"edqian/twitter-climate-change-sentiment-dataset\")\n    csv_path = os.path.join(path, \"twitter_sentiment_data.csv\")\n    df = pd.read_csv(csv_path)\n    df_clean = df.dropna(subset=['message', 'sentiment']).reset_index(drop=True)\n    df_clean = df_clean.drop(columns='tweetid')\n    df_clean = df_clean[df_clean['sentiment'] != 2].reset_index(drop=True)\n    train_df, test_df = train_test_split(df_clean, test_size=0.2, random_state=42, stratify=df_clean['sentiment'])\n    sentiments = sorted(df_clean['sentiment'].unique().tolist())\n    sentiment2id = {sentiment: i for i, sentiment in enumerate(sentiments)}\n    train_df['sentiment_encoded'] = train_df['sentiment'].map(sentiment2id)\n    test_df['sentiment_encoded'] = test_df['sentiment'].map(sentiment2id)\n    train_dataset = Dataset.from_pandas(train_df)\n    test_dataset = Dataset.from_pandas(test_df)\n\n    def tokenize_function(examples):\n        return tokenizer(examples['message'], padding=\"max_length\", truncation=True, max_length=128)\n\n    tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=os.cpu_count())\n    tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, num_proc=os.cpu_count())\n    \n    cols_to_remove = ['sentiment', 'message']\n    if '__index_level_0__' in tokenized_train_dataset.column_names:\n        cols_to_remove.append('__index_level_0__')\n    tokenized_train_dataset = tokenized_train_dataset.remove_columns(cols_to_remove)\n    tokenized_test_dataset = tokenized_test_dataset.remove_columns(cols_to_remove)\n    tokenized_train_dataset = tokenized_train_dataset.rename_column('sentiment_encoded', 'labels')\n    tokenized_test_dataset = tokenized_test_dataset.rename_column('sentiment_encoded', 'labels')\n\n    tokenized_train_dataset.set_format('torch')\n    tokenized_test_dataset.set_format('torch')\n\n    train_sampler = DistributedSampler(tokenized_train_dataset, num_replicas=world_size, rank=rank)\n    eval_sampler = DistributedSampler(tokenized_test_dataset, num_replicas=world_size, rank=rank, shuffle=False)\n    \n    train_dataloader = DataLoader(tokenized_train_dataset, batch_size=batch_size, sampler=train_sampler)\n    eval_dataloader = DataLoader(tokenized_test_dataset, batch_size=batch_size, sampler=eval_sampler)\n\n    optimizer = AdamW(ddp_model.parameters(), lr=2e-5, weight_decay=0.01)\n    num_training_steps = num_epochs * len(train_dataloader)\n    lr_scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n    )\n    \n    # Inisialisasi GradScaler untuk Automatic Mixed Precision (AMP)\n    scaler = GradScaler()\n\n    def compute_metrics(preds, labels):\n        acc = accuracy_score(labels, preds)\n        f1 = f1_score(labels, preds, average=\"weighted\")\n        return {\"accuracy\": acc, \"f1_score\": f1}\n\n    best_accuracy = 0\n    output_dir = \"./roberta-finetuned-ddp-final\"\n    if rank == 0:\n        os.makedirs(output_dir, exist_ok=True)\n\n    for epoch in range(num_epochs):\n        train_sampler.set_epoch(epoch)\n        ddp_model.train()\n        progress_bar_train = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\", disable=(rank!=0))\n\n        for i, batch in enumerate(progress_bar_train):\n            batch = {k: v.to(rank) for k, v in batch.items()}\n            \n            # Menggunakan autocast untuk forward pass (Mixed Precision)\n            with autocast():\n                outputs = ddp_model(**batch)\n                loss = outputs.loss\n                # Normalisasi loss untuk gradient accumulation\n                loss = loss / accumulation_steps\n            \n            # Scale loss dan jalankan backward pass\n            scaler.scale(loss).backward()\n            \n            # Jalankan optimizer step hanya setelah beberapa langkah (Gradient Accumulation)\n            if (i + 1) % accumulation_steps == 0:\n                scaler.step(optimizer)\n                scaler.update()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            if rank == 0:\n                progress_bar_train.set_postfix({'loss': loss.item() * accumulation_steps})\n        \n        # --- Evaluation Loop ---\n        ddp_model.eval()\n        all_val_preds, all_val_labels = [], []\n        with torch.no_grad():\n            for batch in eval_dataloader:\n                batch = {k: v.to(rank) for k, v in batch.items()}\n                with autocast(): # Gunakan autocast juga saat evaluasi\n                    outputs = ddp_model(**batch)\n                all_val_preds.extend(torch.argmax(outputs.logits, dim=-1).cpu().numpy())\n                all_val_labels.extend(batch['labels'].cpu().numpy())\n        \n        # Kumpulkan hasil dari semua GPU\n        pred_tensor = torch.tensor(all_val_preds, dtype=torch.long).to(rank)\n        label_tensor = torch.tensor(all_val_labels, dtype=torch.long).to(rank)\n        gathered_preds = [torch.zeros_like(pred_tensor) for _ in range(world_size)]\n        gathered_labels = [torch.zeros_like(label_tensor) for _ in range(world_size)]\n        dist.all_gather(gathered_preds, pred_tensor)\n        dist.all_gather(gathered_labels, label_tensor)\n\n        if rank == 0:\n            final_preds = torch.cat(gathered_preds).cpu().numpy()\n            final_labels = torch.cat(gathered_labels).cpu().numpy()\n            val_metrics = compute_metrics(final_preds, final_labels)\n            print(f\"Epoch {epoch+1} | Val Acc: {val_metrics['accuracy']:.4f} | Val F1: {val_metrics['f1_score']:.4f}\")\n        \n            print(\"\\n=== Classification Report ===\")\n            print(classification_report(final_labels, final_preds, target_names=[str(s) for s in sentiments]))\n        \n            cm = confusion_matrix(final_labels, final_preds)\n            print(\"\\n=== Confusion Matrix ===\")\n            print(cm)\n        \n            # (Opsional) Visualisasi Confusion Matrix\n            plt.figure(figsize=(6, 5))\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                        xticklabels=[str(s) for s in sentiments],\n                        yticklabels=[str(s) for s in sentiments])\n            plt.xlabel('Predicted')\n            plt.ylabel('Actual')\n            plt.title(f'Confusion Matrix (Epoch {epoch+1})')\n            plt.tight_layout()\n            plt.savefig(f'{output_dir}/confusion_matrix_epoch_{epoch+1}.png')\n            plt.close()\n\n\n            if val_metrics['accuracy'] > best_accuracy:\n                best_accuracy = val_metrics['accuracy']\n                ddp_model.module.save_pretrained(output_dir)\n                tokenizer.save_pretrained(output_dir)\n                print(f\"Model terbaik baru disimpan dengan akurasi: {best_accuracy:.4f}\")\n\n    dist.destroy_process_group()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T07:45:49.964443Z","iopub.execute_input":"2025-10-15T07:45:49.964728Z","iopub.status.idle":"2025-10-15T07:45:49.981081Z","shell.execute_reply.started":"2025-10-15T07:45:49.964706Z","shell.execute_reply":"2025-10-15T07:45:49.980262Z"}},"outputs":[{"name":"stdout","text":"Writing train_script.py\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import torch\nimport torch.multiprocessing as mp\nfrom train_script import main_worker\n\n# Restart session Anda sebelum menjalankan ini jika perlu\n\n# --- KONFIGURASI ---\nmodel_path = \"/kaggle/input/roberta-large-mnli-local/transformers/default/1/roberta-large-mnli-local\"\nnum_epochs = 3\n# Gunakan batch size kecil yang aman untuk memori\nbatch_size = 4\n# Akumulasi gradien untuk mensimulasikan batch size yang lebih besar (4 * 4 = 16)\naccumulation_steps = 4\n# -----------------\n\nworld_size = torch.cuda.device_count()\nprint(f\"Memulai DDP dengan {world_size} GPU.\")\nprint(f\"Batch size per GPU: {batch_size}\")\nprint(f\"Accumulation steps: {accumulation_steps}\")\nprint(f\"Effective global batch size: {batch_size * world_size * accumulation_steps}\")\n\n# Jalankan pelatihan\nmp.spawn(main_worker,\n         args=(world_size, model_path, num_epochs, batch_size, accumulation_steps),\n         nprocs=world_size,\n         join=True)\n\nprint(\"\\nPelatihan DDP selesai.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T07:45:49.983136Z","iopub.execute_input":"2025-10-15T07:45:49.983341Z","iopub.status.idle":"2025-10-15T09:01:54.269047Z","shell.execute_reply.started":"2025-10-15T07:45:49.983326Z","shell.execute_reply":"2025-10-15T09:01:54.268249Z"}},"outputs":[{"name":"stdout","text":"Memulai DDP dengan 2 GPU.\nBatch size per GPU: 4\nAccumulation steps: 4\nEffective global batch size: 32\n","output_type":"stream"},{"name":"stderr","text":"[W1015 07:45:58.209720372 socket.cpp:759] [c10d] The client socket has failed to connect to [localhost]:12355 (errno: 99 - Cannot assign requested address).\n2025-10-15 07:46:00.640897: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760514360.670407      98 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760514360.681038      98 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-10-15 07:46:00.767525: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760514360.785088      99 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760514360.791311      99 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Running DDP on rank 1.\nRunning DDP on rank 0.\n","output_type":"stream"},{"name":"stderr","text":"Map (num_proc=4): 100%|██████████| 27733/27733 [00:06<00:00, 4436.03 examples/s]\nMap (num_proc=4): 100%|██████████| 27733/27733 [00:06<00:00, 4213.05 examples/s]\nMap (num_proc=4): 100%|██████████| 6934/6934 [00:02<00:00, 2969.73 examples/s]\n/kaggle/working/train_script.py:79: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\nEpoch 1/3 [Training]:   0%|          | 0/3467 [00:00<?, ?it/s]/kaggle/working/train_script.py:100: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nMap (num_proc=4): 100%|██████████| 6934/6934 [00:02<00:00, 2925.85 examples/s]\n/kaggle/working/train_script.py:79: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n/kaggle/working/train_script.py:100: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpoch 1/3 [Training]: 100%|█████████▉| 3466/3467 [24:42<00:00,  2.36it/s, loss=0.28]  /kaggle/working/train_script.py:125: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(): # Gunakan autocast juga saat evaluasi\nEpoch 1/3 [Training]: 100%|██████████| 3467/3467 [24:42<00:00,  2.34it/s, loss=0.28]\n/kaggle/working/train_script.py:125: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(): # Gunakan autocast juga saat evaluasi\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Val Acc: 0.8151 | Val F1: 0.7931\n\n=== Classification Report ===\n              precision    recall  f1-score   support\n\n          -1       0.69      0.80      0.74       798\n           0       0.84      0.37      0.52      1543\n           1       0.83      0.97      0.89      4593\n\n    accuracy                           0.82      6934\n   macro avg       0.79      0.71      0.72      6934\nweighted avg       0.82      0.82      0.79      6934\n\n\n=== Confusion Matrix ===\n[[ 640   39  119]\n [ 198  576  769]\n [  88   69 4436]]\nModel terbaik baru disimpan dengan akurasi: 0.8151\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/3 [Training]:   0%|          | 0/3467 [00:00<?, ?it/s]/kaggle/working/train_script.py:100: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpoch 2/3 [Training]: 100%|██████████| 3467/3467 [24:42<00:00,  2.34it/s, loss=0.0275] \n/kaggle/working/train_script.py:125: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(): # Gunakan autocast juga saat evaluasi\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Val Acc: 0.8473 | Val F1: 0.8392\n\n=== Classification Report ===\n              precision    recall  f1-score   support\n\n          -1       0.80      0.76      0.78       798\n           0       0.78      0.57      0.66      1543\n           1       0.87      0.96      0.91      4593\n\n    accuracy                           0.85      6934\n   macro avg       0.82      0.76      0.78      6934\nweighted avg       0.84      0.85      0.84      6934\n\n\n=== Confusion Matrix ===\n[[ 603   93  102]\n [ 106  878  559]\n [  45  154 4394]]\nModel terbaik baru disimpan dengan akurasi: 0.8473\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/3 [Training]:   0%|          | 0/3467 [00:00<?, ?it/s]/kaggle/working/train_script.py:100: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nEpoch 3/3 [Training]: 100%|██████████| 3467/3467 [24:40<00:00,  2.34it/s, loss=0.167]   \n/kaggle/working/train_script.py:125: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(): # Gunakan autocast juga saat evaluasi\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Val Acc: 0.8411 | Val F1: 0.8384\n\n=== Classification Report ===\n              precision    recall  f1-score   support\n\n          -1       0.68      0.87      0.77       798\n           0       0.73      0.60      0.66      1543\n           1       0.90      0.92      0.91      4593\n\n    accuracy                           0.84      6934\n   macro avg       0.77      0.80      0.78      6934\nweighted avg       0.84      0.84      0.84      6934\n\n\n=== Confusion Matrix ===\n[[ 698   43   57]\n [ 229  929  385]\n [  93  295 4205]]\n\nPelatihan DDP selesai.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"!ls -R /kaggle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:01:54.269896Z","iopub.execute_input":"2025-10-15T09:01:54.270672Z","iopub.status.idle":"2025-10-15T09:01:54.452466Z","shell.execute_reply.started":"2025-10-15T09:01:54.270649Z","shell.execute_reply":"2025-10-15T09:01:54.451607Z"}},"outputs":[{"name":"stdout","text":"/kaggle:\ninput  lib  working\n\n/kaggle/input:\nroberta-large-mnli-local  twitter-climate-change-sentiment-dataset\n\n/kaggle/input/roberta-large-mnli-local:\ntransformers\n\n/kaggle/input/roberta-large-mnli-local/transformers:\ndefault\n\n/kaggle/input/roberta-large-mnli-local/transformers/default:\n1\n\n/kaggle/input/roberta-large-mnli-local/transformers/default/1:\nroberta-large-mnli-local\n\n/kaggle/input/roberta-large-mnli-local/transformers/default/1/roberta-large-mnli-local:\nconfig.json  model.safetensors\t      tokenizer_config.json  vocab.json\nmerges.txt   special_tokens_map.json  tokenizer.json\n\n/kaggle/input/twitter-climate-change-sentiment-dataset:\ntwitter_sentiment_data.csv\n\n/kaggle/lib:\nkaggle\n\n/kaggle/lib/kaggle:\ngcp.py\n\n/kaggle/working:\n__pycache__  roberta-finetuned-ddp-final  train_script.py\n\n/kaggle/working/__pycache__:\ntrain_script.cpython-311.pyc\n\n/kaggle/working/roberta-finetuned-ddp-final:\nconfig.json\t\t      merges.txt\t       tokenizer.json\nconfusion_matrix_epoch_1.png  model.safetensors        vocab.json\nconfusion_matrix_epoch_2.png  special_tokens_map.json\nconfusion_matrix_epoch_3.png  tokenizer_config.json\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import shutil\nimport os\n\n# Direktori tempat model Anda disimpan\nmodel_directory = \"./roberta-finetuned-ddp-final\"\n# Nama file ZIP yang akan dibuat\noutput_zip_filename = \"roberta-finetuned-final\"\n\nprint(f\"Mengompres direktori '{model_directory}'...\")\n\n# Membuat file arsip (ZIP)\nshutil.make_archive(output_zip_filename, 'zip', model_directory)\n\nprint(f\"\\nSelesai! Model Anda telah disimpan sebagai '{output_zip_filename}.zip'\")\nprint(f\"Anda bisa menemukan file ini di direktori output: /kaggle/working/{output_zip_filename}.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:01:54.453518Z","iopub.execute_input":"2025-10-15T09:01:54.453762Z","iopub.status.idle":"2025-10-15T09:03:11.473996Z","shell.execute_reply.started":"2025-10-15T09:01:54.453741Z","shell.execute_reply":"2025-10-15T09:03:11.473228Z"}},"outputs":[{"name":"stdout","text":"Mengompres direktori './roberta-finetuned-ddp-final'...\n\nSelesai! Model Anda telah disimpan sebagai 'roberta-finetuned-final.zip'\nAnda bisa menemukan file ini di direktori output: /kaggle/working/roberta-finetuned-final.zip\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom tqdm import tqdm\n\n# Pastikan model di mode evaluasi\nmodel.eval()\n\n# Simpan hasil prediksi dan label\nall_preds = []\nall_labels = []\n\nbatch_size = 8  # kecilkan jika GPU kecil\nnum_batches = int(np.ceil(len(test_df) / batch_size))\n\nwith torch.no_grad():\n    for i in tqdm(range(num_batches), desc=\"Evaluating in batches\"):\n        batch_texts = test_df['message'].iloc[i*batch_size:(i+1)*batch_size].tolist()\n        batch_labels = test_df['sentiment'].iloc[i*batch_size:(i+1)*batch_size].map(sentiment2id).tolist()\n\n        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n        outputs = model(**inputs)\n        preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n\n        all_preds.extend(preds)\n        all_labels.extend(batch_labels)\n\n# Konversi ke array numpy\nall_preds_np = np.array(all_preds)\nall_labels_np = np.array(all_labels)\n\n# Cari indeks prediksi salah\nincorrect_indices = np.where(all_preds_np != all_labels_np)[0]\n\nprint(f\"Number of incorrect predictions: {len(incorrect_indices)}\")\nprint(\"\\nExamples of incorrect predictions:\")\n\nnum_examples_to_show = 10\nfor i in incorrect_indices[:num_examples_to_show]:\n    original_text = test_df.iloc[i]['message']\n    true_label_encoded = all_labels_np[i]\n    predicted_label_encoded = all_preds_np[i]\n\n    true_sentiment = id2sentiment[true_label_encoded]\n    predicted_sentiment = id2sentiment[predicted_label_encoded]\n\n    print(f\"\\nText: {original_text}\")\n    print(f\"True Sentiment: {true_sentiment}, Predicted Sentiment: {predicted_sentiment}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:16:15.276462Z","iopub.execute_input":"2025-10-15T09:16:15.277208Z","iopub.status.idle":"2025-10-15T09:16:57.518974Z","shell.execute_reply.started":"2025-10-15T09:16:15.277182Z","shell.execute_reply":"2025-10-15T09:16:57.518288Z"}},"outputs":[{"name":"stderr","text":"Evaluating in batches: 100%|██████████| 758/758 [00:42<00:00, 17.95it/s]","output_type":"stream"},{"name":"stdout","text":"Number of incorrect predictions: 617\n\nExamples of incorrect predictions:\n\nText: @Libertea2012  The message continues the same, climate change is dangerous to our future, and world wide threat 2 all of us!\nTrue Sentiment: 1, Predicted Sentiment: -1\n\nText: Or global warming!\nTrue Sentiment: -1, Predicted Sentiment: 0\n\nText: Four inches of global warming and counting....\nTrue Sentiment: -1, Predicted Sentiment: 0\n\nText: How are you going to tell me that climate change/global warming isnt real...\nTrue Sentiment: 0, Predicted Sentiment: 1\n\nText: The irrepressible Mark Carney has set up a Stability Board to harass businesses on  'plans for climate change'\nFOR GOD'S\nTrue Sentiment: -1, Predicted Sentiment: 0\n\nText: Rainfall trends in arid regions buck commonly held climate change theories\nTrue Sentiment: 1, Predicted Sentiment: -1\n\nText: @amlozyk Expert - climate change beyond our control\nTrue Sentiment: 0, Predicted Sentiment: 1\n\nText: Obamas New EPA Climate Change Regulations Will Cost 7 Million Jobs For Blacks, 12 Million For Hispanics\nTrue Sentiment: 0, Predicted Sentiment: -1\n\nText: RT @rickgladstone: No mention of climate change in Trump's #UNGA speech.\nTrue Sentiment: 0, Predicted Sentiment: 1\n\nText: Fuck global warming man, my whole style is jacket based, wtf am I supposed to do now that it's 90 in October\nTrue Sentiment: 0, Predicted Sentiment: 1\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def predict_sentiment(text, model, tokenizer, device, id2sentiment, max_length=128):\n    inputs = tokenizer(\n        text,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=max_length\n    )\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n\n    predicted_sentiment_id = predictions.item()\n    predicted_sentiment = id2sentiment[predicted_sentiment_id]\n\n    return predicted_sentiment\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:20:44.096190Z","iopub.execute_input":"2025-10-15T09:20:44.096710Z","iopub.status.idle":"2025-10-15T09:20:44.101802Z","shell.execute_reply.started":"2025-10-15T09:20:44.096689Z","shell.execute_reply":"2025-10-15T09:20:44.100990Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"climate_texts = [\n    \"Climate change is a serious issue and we need to act now!\",\n    \"I don’t believe climate change is real.\",\n    \"The weather is nice today, not too hot or cold.\"\n]\n\nfor text in climate_texts:\n    prediction = predict_sentiment(text, model, tokenizer, device, id2sentiment)\n    print(f\"Text: '{text}'\\nPrediction: {prediction}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T09:21:36.284018Z","iopub.execute_input":"2025-10-15T09:21:36.284651Z","iopub.status.idle":"2025-10-15T09:21:36.387807Z","shell.execute_reply.started":"2025-10-15T09:21:36.284625Z","shell.execute_reply":"2025-10-15T09:21:36.387215Z"}},"outputs":[{"name":"stdout","text":"Text: 'Climate change is a serious issue and we need to act now!'\nPrediction: 1\n\nText: 'I don’t believe climate change is real.'\nPrediction: 0\n\nText: 'The weather is nice today, not too hot or cold.'\nPrediction: 0\n\n","output_type":"stream"}],"execution_count":23}]}